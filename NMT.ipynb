{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "Cw6Rb_tT3sZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow-text\n"
      ],
      "metadata": {
        "id": "dXGxYZ_kBplR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pathlib"
      ],
      "metadata": {
        "id": "TYFA0bQR4AH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = pathlib.Path(\"por.txt\")"
      ],
      "metadata": {
        "id": "lUXIP8UR4EGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "metadata": {
        "id": "msFiToAp4EMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path):\n",
        "    text = path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    lines = text.splitlines()\n",
        "    pairs = [line.split(\"\\t\") for line in lines]\n",
        "\n",
        "    context = np.array([context for target, context, _ in pairs])\n",
        "    target = np.array([target for target, context, _ in pairs])\n",
        "\n",
        "    return context, target"
      ],
      "metadata": {
        "id": "-Rt0sCj_4ER7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_sentences, english_sentences = load_data(path_to_file)"
      ],
      "metadata": {
        "id": "fNAqbAou4EXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = (portuguese_sentences, english_sentences)"
      ],
      "metadata": {
        "id": "_0lKUUfO4Edo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(english_sentences)\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "GGnDr7ad4EkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_train = np.random.uniform(size=(len(portuguese_sentences),)) < 0.8"
      ],
      "metadata": {
        "id": "uQ_vpRra4Er6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_raw = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (english_sentences[is_train], portuguese_sentences[is_train])\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "val_raw = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (english_sentences[~is_train], portuguese_sentences[~is_train])\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "GngkQQru4EzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_lower_and_split_punct(text):\n",
        "    text = tf_text.normalize_utf8(text, \"NFKD\")\n",
        "    text = tf.strings.lower(text)\n",
        "    text = tf.strings.regex_replace(text, \"[^ a-z.?!,¿]\", \"\")\n",
        "    text = tf.strings.regex_replace(text, \"[.?!,¿]\", r\" \\0 \")\n",
        "    text = tf.strings.strip(text)\n",
        "    text = tf.strings.join([\"[SOS]\", text, \"[EOS]\"], separator=\" \")\n",
        "    return text"
      ],
      "metadata": {
        "id": "cBg5buGK4E60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 12000"
      ],
      "metadata": {
        "id": "G5A69vKE4FDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True\n",
        ")"
      ],
      "metadata": {
        "id": "topanbvs4FKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorizer.adapt(train_raw.map(lambda context, target: context))"
      ],
      "metadata": {
        "id": "G3dN9Lt_4FSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_vectorizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct, max_tokens=max_vocab_size, ragged=True\n",
        ")"
      ],
      "metadata": {
        "id": "OHpk5-MD4FbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_vectorizer.adapt(train_raw.map(lambda context, target: target))"
      ],
      "metadata": {
        "id": "wYG8rJNQ4FkK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(context, target):\n",
        "    context = english_vectorizer(context).to_tensor()\n",
        "    target = portuguese_vectorizer(target)\n",
        "    targ_in = target[:, :-1].to_tensor()\n",
        "    targ_out = target[:, 1:].to_tensor()\n",
        "    return (context, targ_in), targ_out"
      ],
      "metadata": {
        "id": "BANSNNVB4Fva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_raw.map(process_text, tf.data.AUTOTUNE)\n",
        "val_data = val_raw.map(process_text, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "7bkzW8Qs4F4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_raw\n",
        "del val_raw"
      ],
      "metadata": {
        "id": "NsXobYPP4GCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_loss(y_true, y_pred):\n",
        "\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "\n",
        "\n",
        "    mask = tf.cast(y_true != 0, loss.dtype)\n",
        "\n",
        "    loss *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "HIL6wuj74GM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def masked_acc(y_true, y_pred):\n",
        "    y_pred = tf.argmax(y_pred, axis=-1)\n",
        "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
        "    match = tf.cast(y_true == y_pred, tf.float32)\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    match*= mask\n",
        "\n",
        "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ],
      "metadata": {
        "id": "apw0tMuv4GWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_text(tokens, id_to_word):\n",
        "    words = id_to_word(tokens)\n",
        "    result = tf.strings.reduce_join(words, axis=-1, separator=\" \")\n",
        "    return result"
      ],
      "metadata": {
        "id": "1OcpQwTM4Gju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMT model with attention"
      ],
      "metadata": {
        "id": "Cy4cHSKFCUJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Setting this env variable prevents TF warnings from showing up\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "G2ZSKX3D4Gyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portuguese_sentences, english_sentences = sentences\n",
        "\n",
        "print(f\"English (to translate) sentence:\\n\\n{english_sentences[-5]}\\n\")\n",
        "print(f\"Portuguese (translation) sentence:\\n\\n{portuguese_sentences[-5]}\")"
      ],
      "metadata": {
        "id": "lUZGJfBV4HBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del portuguese_sentences\n",
        "del english_sentences\n",
        "del sentences"
      ],
      "metadata": {
        "id": "RtuKy2MJ4HNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First 10 words of the english vocabulary:\\n\\n{english_vectorizer.get_vocabulary()[:10]}\\n\")\n",
        "print(f\"First 10 words of the portuguese vocabulary:\\n\\n{portuguese_vectorizer.get_vocabulary()[:10]}\")"
      ],
      "metadata": {
        "id": "aMpbIXbE4Hjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of the vocabulary\n",
        "vocab_size_por = portuguese_vectorizer.vocabulary_size()\n",
        "vocab_size_eng = english_vectorizer.vocabulary_size()\n",
        "\n",
        "print(f\"Portuguese vocabulary is made up of {vocab_size_por} words\")\n",
        "print(f\"English vocabulary is made up of {vocab_size_eng} words\")"
      ],
      "metadata": {
        "id": "x3tWT6BF4IA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This helps you convert from words to ids\n",
        "word_to_id = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\"\n",
        ")\n",
        "\n",
        "# This helps you convert from ids to words\n",
        "id_to_word = tf.keras.layers.StringLookup(\n",
        "    vocabulary=portuguese_vectorizer.get_vocabulary(),\n",
        "    mask_token=\"\",\n",
        "    oov_token=\"[UNK]\",\n",
        "    invert=True,\n",
        ")"
      ],
      "metadata": {
        "id": "6kpergmI4IgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_id = word_to_id(\"[UNK]\")\n",
        "sos_id = word_to_id(\"[SOS]\")\n",
        "eos_id = word_to_id(\"[EOS]\")\n",
        "baunilha_id = word_to_id(\"baunilha\")\n",
        "\n",
        "print(f\"The id for the [UNK] token is {unk_id}\")\n",
        "print(f\"The id for the [SOS] token is {sos_id}\")\n",
        "print(f\"The id for the [EOS] token is {eos_id}\")\n",
        "print(f\"The id for baunilha (vanilla) is {baunilha_id}\")"
      ],
      "metadata": {
        "id": "SFEM7RT84I_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (to_translate, sr_translation), translation in train_data.take(1):\n",
        "    print(f\"Tokenized english sentence:\\n{to_translate[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence (shifted to the right):\\n{sr_translation[0, :].numpy()}\\n\\n\")\n",
        "    print(f\"Tokenized portuguese sentence:\\n{translation[0, :].numpy()}\\n\\n\")"
      ],
      "metadata": {
        "id": "UMcPEBid4JfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ponywNLpEc5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X-gHMyUgEdJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6Fl8r5oEdaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0onTTx2nEdsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WAS6hblmEd--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FTcgR_aEeRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XL64L_j6Eejn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v_h0mi11Ee3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XLrd219PEfMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7BxN-GqGEfhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4lhhzHL4Ef49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0x4swIkYEgQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o0_WlNqdEgoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hF7wFz9FEhHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FVLiPMyNEhkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0nimPJZQEiFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zzrn-oeEii4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U4nkakBUEi9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QC5pauPEjZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SOHliTvYEj2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S8zJp04vEkYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HEG32clBEk2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ilBU_0QElUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrRYYtKoElyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwij_9oPEmTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M0raMKNREm2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_p9kXJmVEngQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDjFIoO_EoLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMGcnJm_Eowu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3ddLFc4EpXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGuSMjS0EqAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TN_MtQKyEqy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p3f4doKwEr33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3wxR6REEtY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}